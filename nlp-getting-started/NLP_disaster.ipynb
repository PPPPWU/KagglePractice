{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from datasets import load_metric\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import mode\n",
    "import os\n",
    "os.environ['WANDB_DISABLED'] = 'True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4549</th>\n",
       "      <td>6466</td>\n",
       "      <td>injured</td>\n",
       "      <td>USA</td>\n",
       "      <td>Offers : http://t.co/Gl3C1vc88P #8392 Deluxe T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4512</th>\n",
       "      <td>6413</td>\n",
       "      <td>hurricane</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The hurricane mixxtail kinda tastes like the w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4368</th>\n",
       "      <td>6203</td>\n",
       "      <td>hijacker</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Complete Solution to Get Rid of http://t.co/9C...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4297</th>\n",
       "      <td>6103</td>\n",
       "      <td>hellfire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@HellFire_eV @JackPERU1 then I do this to one ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6235</th>\n",
       "      <td>8903</td>\n",
       "      <td>snowstorm</td>\n",
       "      <td>Manchester</td>\n",
       "      <td>@Groupon_UK it won't let me as you don't follo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3160</th>\n",
       "      <td>4537</td>\n",
       "      <td>emergency</td>\n",
       "      <td>Southern Maine</td>\n",
       "      <td>Former heroin addict shares story as city lead...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>4191</td>\n",
       "      <td>drown</td>\n",
       "      <td>somewhere in Indiana</td>\n",
       "      <td>Going to go drown my sorrows with sad music brb</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318</th>\n",
       "      <td>3334</td>\n",
       "      <td>demolished</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>ÛÏ@SplottDave: @TeamPalestina That's about 28...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3392</th>\n",
       "      <td>4856</td>\n",
       "      <td>evacuation</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This is an evil generation\\nRock and roll evac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     keyword               location  \\\n",
       "4549  6466     injured                    USA   \n",
       "4512  6413   hurricane                    NaN   \n",
       "4368  6203    hijacker                    NaN   \n",
       "4297  6103    hellfire                    NaN   \n",
       "13      19         NaN                    NaN   \n",
       "6235  8903   snowstorm             Manchester   \n",
       "3160  4537   emergency         Southern Maine   \n",
       "2917  4191       drown  somewhere in Indiana    \n",
       "2318  3334  demolished                Chicago   \n",
       "3392  4856  evacuation                    NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "4549  Offers : http://t.co/Gl3C1vc88P #8392 Deluxe T...       1  \n",
       "4512  The hurricane mixxtail kinda tastes like the w...       0  \n",
       "4368  Complete Solution to Get Rid of http://t.co/9C...       0  \n",
       "4297  @HellFire_eV @JackPERU1 then I do this to one ...       0  \n",
       "13              #Flood in Bago Myanmar #We arrived Bago       1  \n",
       "6235  @Groupon_UK it won't let me as you don't follo...       0  \n",
       "3160  Former heroin addict shares story as city lead...       1  \n",
       "2917    Going to go drown my sorrows with sad music brb       0  \n",
       "2318  ÛÏ@SplottDave: @TeamPalestina That's about 28...       1  \n",
       "3392  This is an evil generation\\nRock and roll evac...       0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train, valid = train_test_split(train, test_size=0.2, random_state=2)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're shaking...It's an earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>They'd probably still show more life than Arse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey! How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a nice hat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fuck off!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n",
       "5  12     NaN      NaN                 We're shaking...It's an earthquake\n",
       "6  21     NaN      NaN  They'd probably still show more life than Arse...\n",
       "7  22     NaN      NaN                                  Hey! How are you?\n",
       "8  27     NaN      NaN                                   What a nice hat?\n",
       "9  29     NaN      NaN                                          Fuck off!"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_token(example, tokenizer=tokenizer):\n",
    "        return tokenizer(example['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6090/6090 [00:00<00:00, 9189.26 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "train_ds = Dataset.from_pandas(train)\n",
    "tokenized_train = train_ds.map(process_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(model_path, train_df, valid_df, test_df):\n",
    "    print(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    train_ds = Dataset.from_pandas(train_df)\n",
    "    valid_ds = Dataset.from_pandas(valid_df)\n",
    "    test_ds = Dataset.from_pandas(test_df)\n",
    "\n",
    "    def process_token(example, tokenizer=tokenizer):\n",
    "        return tokenizer(example['text'])\n",
    "    \n",
    "    tokenized_train = train_ds.map(process_token)\n",
    "    tokenized_valid = valid_ds.map(process_token)\n",
    "    tokenized_test = test_ds.map(process_token)\n",
    "\n",
    "    columns_to_remove = ['id', 'keyword', 'location', '__index_level_0__'] #if __index_level_0__ notice the first index\n",
    "    train_dataset = tokenized_train.remove_columns(columns_to_remove)\n",
    "    valid_dataset = tokenized_valid.remove_columns(columns_to_remove)\n",
    "    columns_to_remove_test = ['id', 'keyword', 'location']\n",
    "    test_dataset = tokenized_test.remove_columns(columns_to_remove_test)\n",
    "\n",
    "    train_dataset = train_dataset.rename_column('target', 'label')\n",
    "    valid_dataset = valid_dataset.rename_column('target', 'label')\n",
    "    return train_dataset, valid_dataset, test_dataset, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    load_acc = load_metric('accuracy')\n",
    "    load_f1 = load_metric('f1')\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    acc = load_acc.compute(predictions=predictions, refereences = labels)['accuracy']\n",
    "    f1 = load_f1.compute(predictions=predictions, references = labels)['f1']\n",
    "    return {'acc':acc, 'f1':f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_trainer(model_path, tokenizer, lr, ep, train_dataset, valid_dataset):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    train_args = TrainingArguments(\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=ep,\n",
    "        per_device_train_batch_size=16,\n",
    "        weight_decay=0.01,\n",
    "        output_dir=model_path\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_fusion:\n",
    "    def __init__(self, model_paths, lr, ep) -> None:\n",
    "        self.model_paths= model_paths\n",
    "        self.learning_rate = lr\n",
    "        self.epoch_num = ep\n",
    "        self.preds =[]\n",
    "\n",
    "    def train_pred_multiple_models(self):\n",
    "        for model_path in self.model_paths:\n",
    "            print(f'training : {model_path}')\n",
    "            print('total:', torch.cuda.get_device_properties(0).total_memory/1e9)\n",
    "            print('allocated:', torch.cuda.memory_allocated(0)/1e9)\n",
    "            print('cached', torch.cuda.memory_reserved(0)/1e9)\n",
    "\n",
    "            train_dataset, valid_dataset, test_dataset, tokenizer = tokenization(model_path, train, valid, test)\n",
    "            trainer = init_trainer(model_path, tokenizer, self.learning_rate, self.epoch_num, train_dataset, valid_dataset)\n",
    "            trainer.train()\n",
    "\n",
    "            print(test_dataset.shape)\n",
    "            prediction = trainer.predict(test_dataset=test_dataset)\n",
    "            print(prediction.predictions.shape)\n",
    "            predictions = np.argmax(a=prediction.predictions, axis=-1)\n",
    "            self.preds.append(model_path, predictions)\n",
    "\n",
    "    def fusion_pred(self):\n",
    "        all_preds = [pred[1] for pred in self.preds]\n",
    "        final_preds = mode(all_preds, axis=0)[0]\n",
    "\n",
    "        return final_preds.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_path = 'roberta-base'\n",
    "deberta_path = 'microsoft/deberta-v3-base'\n",
    "distilbert_path = 'distilbert-base-uncased'\n",
    "model_paths = [distilbert_path,roberta_path,deberta_path]\n",
    "lr = 2e-5\n",
    "ep = 3\n",
    "models = model_fusion(model_paths,lr,ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training : distilbert-base-uncased\n",
      "total: 25.756696576\n",
      "allocated: 0.829641728\n",
      "cached 1.68820736\n",
      "distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 398/1143 [26:54<50:21,  4.06s/it]\n",
      "Map: 100%|██████████| 6090/6090 [00:00<00:00, 8330.56 examples/s]\n",
      "Map: 100%|██████████| 1523/1523 [00:00<00:00, 9620.66 examples/s]\n",
      "Map: 100%|██████████| 3263/3263 [00:00<00:00, 10233.77 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "d:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      " 44%|████▎     | 500/1143 [00:09<00:12, 53.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4138, 'grad_norm': 5.9178643226623535, 'learning_rate': 1.1251093613298338e-05, 'epoch': 1.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1000/1143 [00:20<00:02, 49.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2948, 'grad_norm': 1.9131118059158325, 'learning_rate': 2.502187226596676e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1143/1143 [00:25<00:00, 45.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 25.1196, 'train_samples_per_second': 727.319, 'train_steps_per_second': 45.502, 'train_loss': 0.340983606907535, 'epoch': 3.0}\n",
      "(3263, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [00:01<00:00, 269.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3263, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list.append() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_pred_multiple_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m pred \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mfusion_pred()\n\u001b[0;32m      3\u001b[0m models\u001b[38;5;241m.\u001b[39mpreds\n",
      "Cell \u001b[1;32mIn[45], line 23\u001b[0m, in \u001b[0;36mmodel_fusion.train_pred_multiple_models\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(prediction\u001b[38;5;241m.\u001b[39mpredictions\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     22\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(a\u001b[38;5;241m=\u001b[39mprediction\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: list.append() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "models.train_pred_multiple_models()\n",
    "pred = models.fusion_pred()\n",
    "models.preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer : AutoTokenizer 'pre-train' <br>\n",
    "Dataset : from_pandas()<br>\n",
    "Preprocessing : Dataset.map()<br>\n",
    "init_trainer : AutoModelForSequenceClassification 'pre-train'<br>\n",
    "data_collator : define how to batch data<br>\n",
    "dataset_argument : define the parameters of trainer<br>\n",
    "compute_metrics : define evalua method<br>\n",
    "trainer.train()<br>\n",
    "trainer.predict() - > (n, num_class)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
